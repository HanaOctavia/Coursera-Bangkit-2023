{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes\n",
    "Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
    "\n",
    "* Train a naive bayes model on a sentiment analysis task\n",
    "* Test using your model\n",
    "* Compute ratios of positive words to negative words\n",
    "* Do some error analysis\n",
    "* Predict on your own tweet\n",
    "\n",
    "You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n",
    "* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiment.\n",
    "* This approach gives us simpler formulas for these 2-way classification tasks.\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/supplement/YLuAg/h-ow-to-refresh-your-workspace).\n",
    "\n",
    "Lets get started!\n",
    "\n",
    "Load the cell below to import some packages.\n",
    "You  may want to browse the documentation of unfamiliar libraries and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Importing Functions and Data](#0)\n",
    "- [1 - Process the Data](#1)\n",
    "    - [1.1 - Implementing your Helper Functions](#1-1)\n",
    "        - [Exercise 1 - count_tweets (UNQ_C1)](#ex-1)\n",
    "- [2 - Train your Model using Naive Bayes](#2)\n",
    "    - [Exercise 2 - train_naive_bayes (UNQ_C2)](#ex-2)\n",
    "- [3 - Test your Naive Bayes](#3)\n",
    "    - [Exercise 3 - naive_bayes_predict  (UNQ_C4)](#ex-3)\n",
    "    - [Exercise 4 - test_naive_bayes (UNQ_C6)](#ex-4)\n",
    "- [4 - Filter words by Ratio of Positive to Negative Counts](#4)\n",
    "    - [Exercise 5 - get_ratio (UNQ_C8)](#ex-5)\n",
    "    - [Exercise 6 - get_words_by_threshold (UNQ_C9)](#ex-6)\n",
    "- [5 - Error Analysis](#5)\n",
    "- [6 - Predict with your own Tweet](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Importing Functions and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "import w2_unittest\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook in your local computer,\n",
    "don't forget to download the tweeter samples and stopwords from nltk.\n",
    "\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Process the Data\n",
    "\n",
    "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
    "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
    "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
    "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
    "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
    "\n",
    "We have given you the function `process_tweet` that does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Implementing your Helper Functions\n",
    "\n",
    "To help you train your naive bayes model, you will need to compute a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
    "\n",
    "You will also implement a lookup helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
    "\n",
    "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
    "\n",
    "{\n",
    "    (\"rather\", 1): 2,\n",
    "    (\"happi\", 1) : 1, \n",
    "    (\"excit\", 1) : 1\n",
    "}\n",
    "\n",
    "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
    "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
    "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - count_tweets\n",
    "Create a function `count_tweets` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
    "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
    "- The value the number of times this word appears in the given collection of tweets (an integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Please use the `process_tweet` function that was imported above, and then store the words in their respective dictionaries and sets.</li>\n",
    "    <li>You may find it useful to use the `zip` function to match each element in `tweets` with each element in `ys`.</li>\n",
    "    <li>Remember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.</li>\n",
    "    <li>Assume that the `result` dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented).  It is good practice to check the datatype before incrementing the value, but it's not required here.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 GRADED FUNCTION: count_tweets\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word,y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_count_tweets(count_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Train your Model using Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### So how do you train a Naive Bayes classifier?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared.\n",
    "\n",
    "We will use this dictionary in several parts of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - train_naive_bayes\n",
    "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
    "\n",
    "##### Calculate $V$\n",
    "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
    "\n",
    "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
    "\n",
    "##### Calculate $N_{pos}$, and $N_{neg}$\n",
    "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
    "\n",
    "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
    "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
    "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
    "\n",
    "##### Calculate the logprior\n",
    "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: train_naive_bayes\n",
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)    \n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_pos + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9165\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "0.0\n",
    "\n",
    "9165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "Wrong values for loglikelihood dictionary. Please check your implementation for the loglikelihood dictionary.\n",
      "\u001b[92m 12  Tests passed\n",
      "\u001b[91m 3  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_train_naive_bayes(train_naive_bayes, freqs, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Test your Naive Bayes\n",
    "\n",
    "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - naive_bayes_predict\n",
    "Implement `naive_bayes_predict`.\n",
    "\n",
    "**Instructions**:\n",
    "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
    "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Note\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: naive_bayes_predict\n",
    "\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.0108177258898777\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- The expected output is around 1.55\n",
    "- The sentiment is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_naive_bayes_predict(naive_bayes_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.0108177258898777\n"
     ]
    }
   ],
   "source": [
    "# Experiment with your own tweet.\n",
    "my_tweet = 'He laughed.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - test_naive_bayes\n",
    "Implement test_naive_bayes.\n",
    "\n",
    "**Instructions**:\n",
    "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
    "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
    "* It returns the accuracy of your model.\n",
    "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 GRADED FUNCTION: test_naive_bayes\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Accuracy**:\n",
    "\n",
    "`Naive Bayes accuracy = 0.9955`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> -0.01\n",
      "I am bad -> -0.01\n",
      "this movie should have been great. -> -0.02\n",
      "great -> -0.01\n",
      "great great -> -0.02\n",
      "great great great -> -0.03\n",
      "great great great great -> -0.04\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- I am happy -> 2.14\n",
    "- I am bad -> -1.31\n",
    "- this movie should have been great. -> 2.12\n",
    "- great -> 2.13\n",
    "- great great -> 4.26\n",
    "- great great great -> 6.39\n",
    "- great great great great -> 8.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.021635451779755288"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.unittest_test_naive_bayes(test_naive_bayes, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Filter words by Ratio of Positive to Negative Counts\n",
    "\n",
    "- Some words have more positive counts than others, and can be considered \"more positive\".  Likewise, some words can be considered more negative than others.\n",
    "- One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n",
    "    - Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n",
    "- We can calculate the ratio of positive to negative frequencies of a word.\n",
    "- Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n",
    "- Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - get_ratio\n",
    "Implement get_ratio.\n",
    "\n",
    "- Given the freqs dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n",
    "- Similarly, use the `lookup` function to get the negative count of that word.\n",
    "- Calculate the ratio of positive divided by negative counts\n",
    "\n",
    "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
    "\n",
    "Where pos_words and neg_words correspond to the frequency of the words in their respective classes. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Words</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        Positive word count\n",
    "        </td>\n",
    "         <td>\n",
    "        Negative Word Count\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        glad\n",
    "        </td>\n",
    "         <td>\n",
    "        41\n",
    "        </td>\n",
    "    <td>\n",
    "        2\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        arriv\n",
    "        </td>\n",
    "         <td>\n",
    "        57\n",
    "        </td>\n",
    "    <td>\n",
    "        4\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :(\n",
    "        </td>\n",
    "         <td>\n",
    "        1\n",
    "        </td>\n",
    "    <td>\n",
    "        3663\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :-(\n",
    "        </td>\n",
    "         <td>\n",
    "        0\n",
    "        </td>\n",
    "    <td>\n",
    "        378\n",
    "        </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 GRADED FUNCTION: get_ratio\n",
    "\n",
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
    "    \n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
    "    \n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 162, 'negative': 18, 'ratio': 8.578947368421053}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_get_ratio(get_ratio, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - get_words_by_threshold\n",
    "Implement get_words_by_threshold(freqs,label,threshold)\n",
    "\n",
    "* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n",
    "* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n",
    "* Use the `get_ratio` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n",
    "* Append the `get_ratio` dictionary inside another dictinoary, where the key is the word, and the value is the dictionary `pos_neg_ratio` that is returned by the `get_ratio` function.\n",
    "An example key-value pair would have this structure:\n",
    "```\n",
    "{'happi':\n",
    "    {'positive': 10, 'negative': 20, 'ratio': 0.524}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9 GRADED FUNCTION: get_words_by_threshold\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "        \n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3675, 'ratio': 0.000544069640914037},\n",
       " ':-(': {'positive': 0, 'negative': 386, 'ratio': 0.002583979328165375},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '♛': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '》': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function: find negative words at or below a threshold\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2960, 'negative': 2, 'ratio': 987.0},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':D': {'positive': 523, 'negative': 0, 'ratio': 524.0},\n",
       " ':p': {'positive': 104, 'negative': 0, 'ratio': 105.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 552, 'negative': 0, 'ratio': 553.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " '←': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the positive and negative ratios. Emojis like :( and words like 'me' tend to have a negative connotation. Other words like glad, community, arrives, tend to be found in the positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_get_words_by_threshold(get_words_by_threshold, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model missclassified. Why do you think the missclassifications happened? Were there any assumptions made by your naive bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'bro u wan cut hair anot ur hair long liao bo sinc ord liao take easi lor treat save leav longer :) bro lol sibei xialan'\n",
      "1\t0.00\tb\"back thnx god i'm happi :)\"\n",
      "1\t0.00\tb'thought ear malfunct thank good clear one apolog :-)'\n",
      "1\t0.00\tb'stuck centr right clown right joker left ... :)'\n",
      "1\t0.00\tb'happi friday :-)'\n",
      "1\t0.00\tb'follow :) x'\n",
      "1\t0.00\tb'teenchoic choiceinternationalartist superjunior fight oppa :D'\n",
      "1\t0.00\tb\"birthday today birthday wish hope there' good news ben soon :-)\"\n",
      "1\t0.00\tb'good morn :-) friday  plan day current play shop ...'\n",
      "1\t0.00\tb'happi friday :)'\n",
      "1\t0.00\tb'3 good nigth :) estoy escuchando enemi god'\n",
      "1\t0.00\tb'actual bye bye inde go take drama elsewher :)'\n",
      "1\t0.00\tb'ff readi weekend :) smile'\n",
      "1\t0.00\tb'work :)'\n",
      "1\t0.00\tb':) mood bipolar'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'hahahahahahahahahahahahahaha die :) liter front like'\n",
      "1\t0.00\tb'yoohoo shatter record bajrangibhaijaanstorm superhappi :D'\n",
      "1\t0.00\tb\"who' awak :-)\"\n",
      "1\t0.00\tb'happi friday :-)'\n",
      "1\t0.00\tb'pleasur doll thank wonder energi class :)'\n",
      "1\t0.00\tb'f :)'\n",
      "1\t0.00\tb\"delight m'dear :)\"\n",
      "1\t0.00\tb'hi emma :-) ask bellybutton inni outi'\n",
      "1\t0.00\tb'followback beauti girl today :)'\n",
      "1\t0.00\tb'lyka followback :)'\n",
      "1\t0.00\tb'alrd joy :)'\n",
      "1\t0.00\tb'ourdaughtersourprid dhan dhan satguru tera hi aasra ... mani congratul pita g ... keep bless alway :-)'\n",
      "1\t0.00\tb\"thank ye let' hope work miss :-)\"\n",
      "1\t0.00\tb'keeo guess what behind white cover :) special .. happi bday darl '\n",
      "1\t0.00\tb'happi friday everyon :) big followfriday'\n",
      "1\t0.00\tb'awkward moment name akarshan end stay singl :D foreveralon'\n",
      "1\t0.00\tb'know :) hot alway'\n",
      "1\t0.00\tb\"worri :) know much stress problem site can't wait till event\"\n",
      "1\t0.00\tb'say sweetpea start hold regular gluten free cake pastri workshop :)'\n",
      "1\t0.00\tb'highfiv probabl ahead sinc less artsi verbal :D'\n",
      "1\t0.00\tb'bae :) <3'\n",
      "1\t0.00\tb'kid kaaa :p'\n",
      "1\t0.00\tb'love chat cycl oxford campaign way home yesterday cycl rout pleas :) cycl oxford'\n",
      "1\t0.00\tb'wahoo team :-) masaantoday'\n",
      "1\t0.00\tb'anchor sea .. :)'\n",
      "1\t0.00\tb'dat rp tho thank much guy celebr one month partnership ty raid :D'\n",
      "1\t0.00\tb'visit robbenisland cultur experi tri whale watch aquat safari either great :)'\n",
      "1\t0.00\tb'least friday :D'\n",
      "1\t0.00\tb'mayb garru liara rest crew :D'\n",
      "1\t0.00\tb'thank follow great day :)'\n",
      "1\t0.00\tb'massag appoint avail tomorrow morn burnley give us call book :) 01282 453 110'\n",
      "1\t0.00\tb'great hear nice day thank visit :)'\n",
      "1\t0.00\tb'bore everyth :D'\n",
      "1\t0.00\tb'drama korea 49 day :)'\n",
      "1\t0.00\tb'btw anyon repres footbal manag commun youtub :) fm15 fmfamili vidcon 2015'\n",
      "1\t0.00\tb'mean aamir khan one :)'\n",
      "1\t0.00\tb'happi friday :)'\n",
      "1\t0.00\tb'right back excel job difficult medium :)'\n",
      "1\t0.00\tb':) want ff'\n",
      "1\t0.00\tb'tweet :p'\n",
      "1\t0.00\tb'hey come see us summer fine food fair sunday 11am 4pm ride school :)'\n",
      "1\t0.00\tb'pleas follow liam :)'\n",
      "1\t0.00\tb'love write :) absolut ador nva lucki enough live 5 minuet away roll gamec'\n",
      "1\t0.00\tb'ff thank brian :)'\n",
      "1\t0.00\tb\"headrest loos that' reason pit :-)\"\n",
      "1\t0.00\tb\"hi i'v spoken store advis paypoint card top :) thank beth\"\n",
      "1\t0.00\tb'thank help :-)'\n",
      "1\t0.00\tb'deepthroat good movi :)'\n",
      "1\t0.00\tb'hallo twitter :D'\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'thank wish :)'\n",
      "1\t0.00\tb'aitor sn would mind check bio particip ps4 giveaway could winner :)'\n",
      "1\t0.00\tb'hi jawad today thank follow retweet much appreci :)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'actual need stop tweet drive :-)'\n",
      "1\t0.00\tb'kind feedback truli made day perfect way end work week :)'\n",
      "1\t0.00\tb\"well that' presal ticket bought thank wonder take book ... thing parent :)\"\n",
      "1\t0.00\tb'hello :) get youth job opportun follow'\n",
      "1\t0.00\tb'hope goe well :-)'\n",
      "1\t0.00\tb'betcha :) dumb butt :)'\n",
      "1\t0.00\tb'kik qualki 808 kik kikmenow milf like 4like bore summer sexysaturday :)'\n",
      "1\t0.00\tb'follow :-)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb\"ye hun i'm good :)\"\n",
      "1\t0.00\tb'enjoy :)'\n",
      "1\t0.00\tb'wow vw bu lot umpfff love :)'\n",
      "1\t0.00\tb'ca retweet domg nanti difollow :D'\n",
      "1\t0.00\tb'wish :D :D :D :D :D :D :D kid'\n",
      "1\t0.00\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "1\t0.00\tb'ye car :)'\n",
      "1\t0.00\tb'like especi klee one :-)'\n",
      "1\t0.00\tb'look amaz april love glass :D'\n",
      "1\t0.00\tb'know :)'\n",
      "1\t0.00\tb'build hous :)'\n",
      "1\t0.00\tb'yeah kinda feel like warm butter :D'\n",
      "1\t0.00\tb'realli week big decis feel rather badass :)'\n",
      "1\t0.00\tb\"i'm go sleep hope wake follow luke hem someday :) goodnight fam ili 5so lot \"\n",
      "1\t0.00\tb'congrad better updat page accomplish :-) keep ...'\n",
      "1\t0.00\tb'film day invit reason lol :)'\n",
      "1\t0.00\tb':)'\n",
      "1\t0.00\tb'whenev sister see cri text ask im okay aw someon care :-)'\n",
      "1\t0.00\tb'welcom kfcroleplay enjoy let friend :)'\n",
      "1\t0.00\tb't-shirt :D awesom'\n",
      "1\t0.00\tb'belat .. thank thank :D'\n",
      "1\t0.00\tb\"can't wait see :-)\"\n",
      "1\t0.00\tb'happi happi birthday happi happi birthday :)'\n",
      "1\t0.00\tb'take care :)'\n",
      "1\t0.00\tb'road trip tregaron view potenti new boar finger cross bring boy home :)'\n",
      "1\t0.00\tb':) good know :)'\n",
      "1\t0.00\tb'told im gorgeou extrem sweati walk work made day :)'\n",
      "1\t0.00\tb'morn em :)'\n",
      "1\t0.00\tb'glyon :D'\n",
      "1\t0.00\tb'   definit perfect would pictur next :)'\n",
      "1\t0.00\tb\"initi tee' pm order :)\"\n",
      "1\t0.00\tb'confess :) nice'\n",
      "1\t0.00\tb'welcom :)'\n",
      "1\t0.00\tb'nice dave :D'\n",
      "1\t0.00\tb\"know dumb think johnni rock u seen utub video' mad respect :D\"\n",
      "1\t0.00\tb'sorri loss hope goe well :)'\n",
      "1\t0.00\tb'combin three add bit asian u got truli :)'\n",
      "1\t0.00\tb'happi birthday :)'\n",
      "1\t0.00\tb'got call end pigeon home safe :) definit worth contact'\n",
      "1\t0.00\tb'morn hospit day today hope final get sign long year pain op fingerscross :)'\n",
      "1\t0.00\tb'best photobomb :D pic '\n",
      "1\t0.00\tb'thank se :)'\n",
      "1\t0.00\tb'hello :) get youth job opportun follow'\n",
      "1\t0.00\tb'finish day 33 plan bibl 90 day check 2 chronicl 23 complet :)'\n",
      "1\t0.00\tb'yo gimm month free bill sinc servic pretti much everi day :)'\n",
      "1\t0.00\tb'stat day arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb\"hi i'm definetli exit i'm go new york :)\"\n",
      "1\t0.00\tb'saturday :)'\n",
      "1\t0.00\tb\"i'v type wors thing good :p\"\n",
      "1\t0.00\tb'beat da beat sit well includ new video :-) mobilegam io 8 android'\n",
      "1\t0.00\tb\"i'm play brain dot braindot\"\n",
      "1\t0.00\tb'thank :) '\n",
      "1\t0.00\tb'soon ... :)'\n",
      "1\t0.00\tb'sure :)'\n",
      "1\t0.00\tb'look fun kik thencerest 547 kik kiksext sex followback l4l indiemus kikgirl :)'\n",
      "1\t0.00\tb'goodnight guy :-) rememb tomorrow brand new day fresh start anoth chanc'\n",
      "1\t0.00\tb'bom-dia :) apod ultraviolet ring m31 2015 jul 24'\n",
      "1\t0.00\tb\"i'm play brain dot braindot\"\n",
      "1\t0.00\tb'thank follow great day :)'\n",
      "1\t0.00\tb'oooh love hope fantast time :)'\n",
      "1\t0.00\tb'yawn good morn everyon wag tail everyon today :D'\n",
      "1\t0.00\tb'congrat launch :D'\n",
      "1\t0.00\tb'happi wonder birthday love  beauti day world :)'\n",
      "1\t0.00\tb'ftw :)'\n",
      "1\t0.00\tb'ah see song prefer take maman :)'\n",
      "1\t0.00\tb'hi hayley :-) ask bellybutton inni outi'\n",
      "1\t0.00\tb'gud afterznoon jumma mubarak tweeep :-) plz rememb us ur prayer'\n",
      "1\t0.00\tb'welcom bomb famili :) x'\n",
      "1\t0.00\tb'abp news ka articl read kiya :D bajrangibhaijaanhighestweek 1'\n",
      "1\t0.00\tb'like hurt feel anyth right :)'\n",
      "1\t0.00\tb'check new van outsid olymp park anniversari game :)'\n",
      "1\t0.00\tb'everi year august get fever :p dunno '\n",
      "1\t0.00\tb\"yeah classi look :-) i'v seen peopl attach sort life save equip bed fix 0\"\n",
      "1\t0.00\tb'trend .. follow bobbl link :)'\n",
      "1\t0.00\tb\"next year sure though :) there' 2 girl piss\"\n",
      "1\t0.00\tb'stand cool :)'\n",
      "1\t0.00\tb'movi key life japanes version interest storylin :)'\n",
      "1\t0.00\tb'thank anu :)'\n",
      "1\t0.00\tb'also new layout new :)'\n",
      "1\t0.00\tb'im tri l mh3 english patch psp :D'\n",
      "1\t0.00\tb\"want tweet someth thought i'll probabl lose lot liam girl mutual :)\"\n",
      "1\t0.00\tb'1 day go 1 day left sponsor us matter small donat differ make huge :)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'follow :)'\n",
      "1\t0.00\tb\"hey emili love work i'd like contribut huffpost busi what' best way contact :)\"\n",
      "1\t0.00\tb\"asshol  that' honest tribut :)\"\n",
      "1\t0.00\tb'h_eartshapedbox first leg magictrikband tour go well :D music band rock magictrik tour '\n",
      "1\t0.00\tb'tower ladi :) tb eid'\n",
      "1\t0.00\tb\"whatsapp roommat want anyth pari french man :) lol sure i'll head park grab one\"\n",
      "1\t0.00\tb'great weekend tami thank :)'\n",
      "1\t0.00\tb'love :-)'\n",
      "1\t0.00\tb'want look good :)'\n",
      "1\t0.00\tb'sure :)'\n",
      "1\t0.00\tb'excit weekend :-)'\n",
      "1\t0.00\tb'b3dk far 7an ank mi 15 :)'\n",
      "1\t0.00\tb'god give us strength purpos help other struggl :-)'\n",
      "1\t0.00\tb'lol meant lucki eagl thank servic oceana :-)'\n",
      "1\t0.00\tb\"woke feel incred sick idk caus drank starbuck 11 o'clock last night reaction med :)\"\n",
      "1\t0.00\tb\"oop ... that' call fridayfauxpa :) i'll get chang right thank g\"\n",
      "1\t0.00\tb'fulfil fantasi :)   '\n",
      "1\t0.00\tb'call night go sleep :)'\n",
      "1\t0.00\tb\"yeah they'r give subtl hint also tip 2 use commun last year :-) prim  algorithm iii\"\n",
      "1\t0.00\tb'funni ye soon alway go soon :D'\n",
      "1\t0.00\tb'thank :) get retweet x'\n",
      "1\t0.00\tb'smile receiv text dad say suck :)'\n",
      "1\t0.00\tb'wat :)'\n",
      "1\t0.00\tb\"can't wait rosa love xma :) welcom back ann hope u good one back time :) x\"\n",
      "1\t0.00\tb'morn tweep new post today pleas like share see thank :)'\n",
      "1\t0.00\tb'yvw great day weekend :D'\n",
      "1\t0.00\tb'awww here quick workout boost :-)'\n",
      "1\t0.00\tb'lol :D know idea :p coz unforgett sens humor ;) :) :D love bro'\n",
      "1\t0.00\tb'stat day arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb\"mum' home ... hahahhaah thank lord :)\"\n",
      "1\t0.00\tb'safe :D'\n",
      "1\t0.00\tb'girl :)'\n",
      "1\t0.00\tb\"sombrero high okay i'm lost :)\"\n",
      "1\t0.00\tb\"hellooo i'm rose nice meet :)\"\n",
      "1\t0.00\tb'thank screenshot po let take care spammer :) proceed c ...'\n",
      "1\t0.00\tb'perfect thing pick :) wsalelov'\n",
      "1\t0.00\tb'thank amaz job :)'\n",
      "1\t0.00\tb'die actual pretti entertain :p'\n",
      "1\t0.00\tb\"congrat 100k man can't wait till hit mileston :)\"\n",
      "1\t0.00\tb'morn ... judith answer district council relat question today :-)'\n",
      "1\t0.00\tb'see tomorrow :)'\n",
      "1\t0.00\tb'saw comment midar gender unknown like :) yeah keep say'\n",
      "1\t0.00\tb'wow realli hate :)'\n",
      "1\t0.00\tb'watch saw dan :)'\n",
      "1\t0.00\tb'thank :-) ilysm xx'\n",
      "1\t0.00\tb'thank trade :)'\n",
      "1\t0.00\tb'look forward :-) ks'\n",
      "1\t0.00\tb'lesley :)'\n",
      "1\t0.00\tb\"want protect relationship other i'll talk ppl need know awesom :)\"\n",
      "1\t0.00\tb'oh dang zen look outsid space neat :-)'\n",
      "1\t0.00\tb'look good outfit :)'\n",
      "1\t0.00\tb\"hiya i'm afraid we'v alreadi pick rider year we'll look next year :)\"\n",
      "1\t0.00\tb'good morn u :)'\n",
      "1\t0.00\tb'ha ha ok honey go ahead fyi still dig ya enjoy vote watch side :)'\n",
      "1\t0.00\tb'lucki :)'\n",
      "1\t0.00\tb'learn new skill take time perfect master thank favourit :)'\n",
      "1\t0.00\tb\"       i'v talk u coupl time u seem nice funni :)\"\n",
      "1\t0.00\tb'agre suspici mind :)'\n",
      "1\t0.00\tb'good afternoon :)'\n",
      "1\t0.00\tb'whoop whoop bring fun :)'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb\"say someth i'm give :) i'm sorri get :D\"\n",
      "1\t0.00\tb'good night :)'\n",
      "1\t0.00\tb\"there' gonna anoth one final :)\"\n",
      "1\t0.00\tb'welcom jessica enjoy :)'\n",
      "1\t0.00\tb'best friend carina one san francisco let know want get touch :)'\n",
      "1\t0.00\tb'thank mommi teret :) :) potassium rehydr drinkitallup thirstquench'\n",
      "1\t0.00\tb\"i'm sure tapir calf one like adventur mealtim :)\"\n",
      "1\t0.00\tb'uhc box larg scale one network server :p'\n",
      "1\t0.00\tb\"ever areal we'll lunch dinner :)\"\n",
      "1\t0.00\tb\"here' one extremesport fan among love quadbik get touch :) bloggersrequir bloggersw\"\n",
      "1\t0.00\tb'thank ... love cake :) bear hug end'\n",
      "1\t0.00\tb'masa nowaday brainer type invest list mse fund :)'\n",
      "1\t0.00\tb'magnific :-)'\n",
      "1\t0.00\tb'love work tokyo :) kunoriforceo ceo 1month'\n",
      "1\t0.00\tb'yeah tri look x :)'\n",
      "1\t0.00\tb'welcom kfcroleplay enjoy let friend :)'\n",
      "1\t0.00\tb':) hehe get think mean need sleep nooowww nite nite :)'\n",
      "1\t0.00\tb'hi 5 back :) :)'\n",
      "1\t0.00\tb'ye omg lile tid tmi :)'\n",
      "1\t0.00\tb'may deploy degre sarcasm :)'\n",
      "1\t0.00\tb'cute boy jule :-)'\n",
      "1\t0.00\tb\"can't wait week :D\"\n",
      "1\t0.00\tb'thank follow us betti miller new friend alway welcom :)'\n",
      "1\t0.00\tb'person small thing u expect person u love :)'\n",
      "1\t0.00\tb'ye hddc also favourit :) salman khan movi etern case watch pthht'\n",
      "1\t0.00\tb'oh shoot well watch :D'\n",
      "1\t0.00\tb'far lfc fan make expert spot mental weak lack consist :-)'\n",
      "1\t0.00\tb'tope :)'\n",
      "1\t0.00\tb'good night sweet dream xxoo   '\n",
      "1\t0.00\tb'littl finger :D'\n",
      "1\t0.00\tb'love :) 13 photo explain ukrain russia'\n",
      "1\t0.00\tb'friday :)'\n",
      "1\t0.00\tb'hey fam vote alreadi :) gotta win boy teenchoic'\n",
      "1\t0.00\tb'hi jane silver-wash fritillari :)'\n",
      "1\t0.00\tb'pleasur :)'\n",
      "1\t0.00\tb'great day hunni :)'\n",
      "1\t0.00\tb'good night moon :) pandora'\n",
      "1\t0.00\tb'2 ap studi hall haha great :)'\n",
      "1\t0.00\tb'follow everyon back teamfollowback :-)'\n",
      "1\t0.00\tb\"ha almost gave fuck :) boy trash wast time that' sure\"\n",
      "1\t0.00\tb'amaz pic today :D'\n",
      "1\t0.00\tb'ye alreadi ad pleas check final design :)'\n",
      "1\t0.00\tb'noth much need grow :)'\n",
      "1\t0.00\tb'easi clever interest audienc welcom :)'\n",
      "1\t0.00\tb'great video :D'\n",
      "1\t0.00\tb\"ask end .. i'm friend :)\"\n",
      "1\t0.00\tb\"thank' follow keven :)\"\n",
      "1\t0.00\tb'ha love pic clearli much learn popular english pastim :)'\n",
      "1\t0.00\tb'ashramcal return :p'\n",
      "1\t0.00\tb'catch fall fall ontrack :)'\n",
      "1\t0.00\tb'alway make thing better :)  '\n",
      "1\t0.00\tb\"guess they'll show german also subtitl :)\"\n",
      "1\t0.00\tb'pinter :-)'\n",
      "1\t0.00\tb'goood morninggg jummah mubarak everyon bless amaz day ahead :)'\n",
      "1\t0.00\tb'cute :-) '\n",
      "1\t0.00\tb'stat day arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb'fabul time miss :) xx'\n",
      "1\t0.00\tb'noth perfect smile :)'\n",
      "1\t0.00\tb'never show much :)'\n",
      "1\t0.00\tb':-) cheer pete listen later  '\n",
      "1\t0.00\tb\"that' build sunday :p\"\n",
      "1\t0.00\tb'direct attent ... awesome-o :)'\n",
      "1\t0.00\tb'get us tune well app multipl option :D'\n",
      "1\t0.00\tb'engin :)'\n",
      "1\t0.00\tb'happi friday :D'\n",
      "1\t0.00\tb'cya week :D'\n",
      "1\t0.00\tb'happi shop :)'\n",
      "1\t0.00\tb'fantast show harrog home gift back offic 2 day jet see wonder supplier :)'\n",
      "1\t0.00\tb'nice one :D'\n",
      "1\t0.00\tb\"ye let' :)\"\n",
      "1\t0.00\tb'7 :) :) :) :)'\n",
      "1\t0.00\tb'australia morn :D'\n",
      "1\t0.00\tb'req star :) fridayloug'\n",
      "1\t0.00\tb'knew whole class cri hurt word :)'\n",
      "1\t0.00\tb'opportun anyon ask question babe includ :) 4thstreetmus'\n",
      "1\t0.00\tb'lunch :)'\n",
      "1\t0.00\tb\"i'll hawaii decemb we'll kick back miss :)\"\n",
      "1\t0.00\tb'two 17 year old deepli love :-)'\n",
      "1\t0.00\tb\"aha that' alright   love :)\"\n",
      "1\t0.00\tb'hope nate notic edit sinc made icon :)'\n",
      "1\t0.00\tb'send john@timney.eclipse.co.uk :)'\n",
      "1\t0.00\tb'hah ... thousand lie :D'\n",
      "1\t0.00\tb\"i'd will tri :)\"\n",
      "1\t0.00\tb'read newspap :D'\n",
      "1\t0.00\tb'afternoon date <3 :)'\n",
      "1\t0.00\tb\"here' fan made art u :) guy phone lew bend :D\"\n",
      "1\t0.00\tb'nah beaut way :)'\n",
      "1\t0.00\tb'fallout 4 get see glow sea technic fallout 3 play see :D'\n",
      "1\t0.00\tb\"i'm friend cousin karen gunderson watch europa report thoroughli impress want say great script :-)\"\n",
      "1\t0.00\tb'dri start wale wait duck overtak motorway :-)'\n",
      "1\t0.00\tb'good morn fab friday :)'\n",
      "1\t0.00\tb'hahaha ... thu ... sneak away log niteflirt :D'\n",
      "1\t0.00\tb\"yep we'r trash af im indonesia hbu :-)\"\n",
      "1\t0.00\tb'obvious get better tweet :-)'\n",
      "1\t0.00\tb'welcom kfcroleplay enjoy let friend :)'\n",
      "1\t0.00\tb'bowl alway best friend :)'\n",
      "1\t0.00\tb\"hope i'll :) x\"\n",
      "1\t0.00\tb'say must chri elli :)'\n",
      "1\t0.00\tb'ye friendli contest loser buy drink :)'\n",
      "1\t0.00\tb'thank follow great day :)'\n",
      "1\t0.00\tb'thank :-)'\n",
      "1\t0.00\tb'thank :D'\n",
      "1\t0.00\tb'day practic song :)'\n",
      "1\t0.00\tb\"here' remind need one :-)\"\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :D'\n",
      "1\t0.00\tb'select other .. get :)'\n",
      "1\t0.00\tb\"... play al n he'll score fuck night :)\"\n",
      "1\t0.00\tb'bff .. :)'\n",
      "1\t0.00\tb\"hi here' vid stydia take look :)\"\n",
      "1\t0.00\tb'im send alex like million pictur :)'\n",
      "1\t0.00\tb'love :)'\n",
      "1\t0.00\tb'visit blog thank :D'\n",
      "1\t0.00\tb'happi birthday nawazuddin siddiqu ... big big fan :)'\n",
      "1\t0.00\tb'ye pleas would brighten night shift nomnomnom :D chocol dukefreebiefriday xx'\n",
      "1\t0.00\tb'done eonni :)'\n",
      "1\t0.00\tb'fantast night :)'\n",
      "1\t0.00\tb':)'\n",
      "1\t0.00\tb'boom date :D z'\n",
      "1\t0.00\tb'follow alreadi thank :)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'happi end insyaallah :)'\n",
      "1\t0.00\tb'that way see west ham shit small club london villa 3rd biggest brum :)'\n",
      "1\t0.00\tb'deni vagina rli :D poor thing :D'\n",
      "1\t0.00\tb'smile sunnah :)  '\n",
      "1\t0.00\tb'hi see u like fourfivesecond think u might like deaf ear plz let know u think :)'\n",
      "1\t0.00\tb'share readi :)'\n",
      "1\t0.00\tb\"oh white rabbit cutest thing i'v ever seen wait ador :)\"\n",
      "1\t0.00\tb'welcom :) love weekend izzi'\n",
      "1\t0.00\tb'happi birthday mitch :-) wish best good day'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'done ya minn :) cjradacomateada'\n",
      "1\t0.00\tb'welcom fun :)'\n",
      "1\t0.00\tb'recently.websit upgrad might see new pictur coolingtow soon.thank showinginterest :)'\n",
      "1\t0.00\tb'flipkartfashionfriday multicolor maxi dress pair wid wedg make ur life bright color :)'\n",
      "1\t0.00\tb'alway motiv :-)'\n",
      "1\t0.00\tb'nnnnot see upsid :-)'\n",
      "1\t0.00\tb\"haha kyle gf' babi :)\"\n",
      "1\t0.00\tb'lol ok watch ya bit :p'\n",
      "1\t0.00\tb\"bluesidemenxix i'd love win one :)\"\n",
      "1\t0.00\tb'happi birthday :)'\n",
      "1\t0.00\tb'hello :) ardent follow'\n",
      "1\t0.00\tb'goood mooorn :) want sleep ...'\n",
      "1\t0.00\tb'ghost bae :D love wuppert tagsforlikesapp instagood smile follow cute photooftheday '\n",
      "1\t0.00\tb'hahaha okay thank :)'\n",
      "1\t0.00\tb'lose :D'\n",
      "1\t0.00\tb'hehe cool :)'\n",
      "1\t0.00\tb\"morn u today u got plan lazi i'm still bed toast fresh coffe 2 :) great fridayfunday\"\n",
      "1\t0.00\tb'welcom famili :)'\n",
      "1\t0.00\tb're-sign :)'\n",
      "1\t0.00\tb\"meet harri that' need life :-)\"\n",
      "1\t0.00\tb'stat week arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb'one sleep til wed :-)'\n",
      "1\t0.00\tb'excit chalkhil park celebr tomorrow midday come join fun :-)'\n",
      "1\t0.00\tb'hello :) love carter :)'\n",
      "1\t0.00\tb'good read :)'\n",
      "1\t0.00\tb':-)'\n",
      "1\t0.00\tb'enjoy hope weather hold guy :-)'\n",
      "1\t0.00\tb'done :)'\n",
      "1\t0.00\tb'remedi media tell :-)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'jumma mubarak stay bless :)'\n",
      "1\t0.00\tb\"we'r look forward abl spend time :) enjoy iflix :)\"\n",
      "1\t0.00\tb'tgif weekend ... enjoy :)'\n",
      "1\t0.00\tb'im thank someon like life inspir better everyday love mind follow :)'\n",
      "1\t0.00\tb'becom better atrack better :)'\n",
      "1\t0.00\tb'first third look promis els fail make decis go :)'\n",
      "1\t0.00\tb'crazi girlfriend like :-) (-: jesu christ'\n",
      "1\t0.00\tb\"morn :) gd day store i'm badminton 2 littl'un fabfriday\"\n",
      "1\t0.00\tb'kid insid want one :-)'\n",
      "1\t0.00\tb'imran khan hero :) realli ikprideofpak'\n",
      "1\t0.00\tb'janjua friend :)'\n",
      "1\t0.00\tb'sorri fuck eric think he funni :)'\n",
      "1\t0.00\tb'say bday :)'\n",
      "1\t0.00\tb'hey :) check pictur think :) notic follow pleas  x'\n",
      "1\t0.00\tb':) :) :) pimpl :) :) :) forehead :) :) size :) :) :) volcano :)'\n",
      "1\t0.00\tb'good luck coupl get marri weekend :)'\n",
      "1\t0.00\tb'back :) done na mag miryenda :)'\n",
      "1\t0.00\tb'wait bro :-)'\n",
      "1\t0.00\tb\"parti 3d printer :) celebr futur technology' possibl touchtoday \"\n",
      "1\t0.00\tb'u use app listen music ye app :-)  idownload'\n",
      "1\t0.00\tb'get 25ish snowbal :p'\n",
      "1\t0.00\tb'cours :)'\n",
      "1\t0.00\tb'nd go expir next year dat mtn 6gb u :D realli like'\n",
      "1\t0.00\tb'happi birthday nice day :) loveu'\n",
      "1\t0.00\tb'wanna go back time everyth still fine :-)'\n",
      "1\t0.00\tb'celebr life :) summer morefuninthephilippin laho island caramoan island'\n",
      "1\t0.00\tb'nice quot :) kunoriforceo ceo 1month'\n",
      "1\t0.00\tb'good stuff :) love bit salmon'\n",
      "1\t0.00\tb'jumma kareem :) friday remind recit surah kahaf ..'\n",
      "1\t0.00\tb\"i'm join right :D\"\n",
      "1\t0.00\tb'sure .. :)'\n",
      "1\t0.00\tb\"hi melani :-) fact they'r realli use case normal wifi could issu mention ...\"\n",
      "1\t0.00\tb\"quit phenomen offer bosch wash machin week' definit last long :)\"\n",
      "1\t0.00\tb\"one ok i'll run photo moment :D\"\n",
      "1\t0.00\tb'thank refollow :)'\n",
      "1\t0.00\tb'slept less 7 hour last night prepar possibl nap :p'\n",
      "1\t0.00\tb'idk :p follow   '\n",
      "1\t0.00\tb'feel like relaps :-)'\n",
      "1\t0.00\tb'girl nice wallet boy prada hai .. girl rich .. boy stupid mean bhai ka hai :p prada punjabiswillgetit'\n",
      "1\t0.00\tb' morn babe :)'\n",
      "1\t0.00\tb'lol sorri ultor first hitter ... favor much :)'\n",
      "1\t0.00\tb'finish mass effect 1 yesterday dont know shoud start mass effect 2 100 game :)'\n",
      "1\t0.00\tb'bad would remind exercis 1:12 :-) miss need come back'\n",
      "1\t0.00\tb'add kik ughtm 545 kik kikmeguy kissm nude likeforfollow musicbiz sexysasunday :)'\n",
      "1\t0.00\tb'overwhelm groupmat thank group got 1 75 recit :)'\n",
      "1\t0.00\tb'ha mistak thank point :)'\n",
      "1\t0.00\tb'kyunk aitchison hai :p look respons origin tweet know brother'\n",
      "1\t0.00\tb'snapchat sexyjudi 19 snapchat kikmeboy tagsforlik pussi gay indiemus sexo :)'\n",
      "1\t0.00\tb\"that' true inde curvi rout :)\"\n",
      "1\t0.00\tb'come chill fire :)'\n",
      "1\t0.00\tb'mont doa header :D'\n",
      "1\t0.00\tb'haha x :D'\n",
      "1\t0.00\tb'ted speaker say stress good bodi anoth speaker introduc way avoid stress :) ted thought laboratori :)'\n",
      "1\t0.00\tb'thing note awesom :-)'\n",
      "1\t0.00\tb'might lose friend today know idc :)'\n",
      "1\t0.00\tb'good morn :D lol realli fuckin dark gonna rain hard coupl minut'\n",
      "1\t0.00\tb'far lfc fan make expert spot mental weak lack consist :-)'\n",
      "1\t0.00\tb'ff amaz singer actress model :-)'\n",
      "1\t0.00\tb'happi birthday see tonight :)'\n",
      "1\t0.00\tb\"i'm amsterdam guy wooo :D\"\n",
      "1\t0.00\tb'stuff happen :)'\n",
      "1\t0.00\tb'friend :)'\n",
      "1\t0.00\tb'neobyt :) univers pictur seed pirat movi copi file takedown notic'\n",
      "1\t0.00\tb'2 indirag want judiciari commit govt polici want rbi commit know mani similar view :)'\n",
      "1\t0.00\tb\"thought' kepler 452b :D\"\n",
      "1\t0.00\tb'progress stone transfer chelsea :D'\n",
      "1\t0.00\tb'gg bro :) fun seri'\n",
      "1\t0.00\tb'alway part part defenit ...  :p'\n",
      "1\t0.00\tb'cours man :) nofx shit'\n",
      "1\t0.00\tb'friskyfiday yipee :) x'\n",
      "1\t0.00\tb'morn kim sorri hear shed light issu look :) helen'\n",
      "1\t0.00\tb'great incent get kid eat fruit vege :-)'\n",
      "1\t0.00\tb'welcom sir :)'\n",
      "1\t0.00\tb'know averag take 6 second read god dam mother fuck tweet :)'\n",
      "1\t0.00\tb'thank favourit tweet marin :)'\n",
      "1\t0.00\tb\"get close can't wait new season :)\"\n",
      "1\t0.00\tb'gz :D name'\n",
      "1\t0.00\tb'morn day :-)'\n",
      "1\t0.00\tb'guess sleep list thing today :-)'\n",
      "1\t0.00\tb'hai rajeev .. hope u r hvng funfil friday.it ws reali amaz c diff side kabir.fel luv u :) smile pl'\n",
      "1\t0.00\tb'prettier :) mine :)'\n",
      "1\t0.00\tb'happi birthday best wish dresden germani :) love <3 <3 <3 <3 pleas share'\n",
      "1\t0.00\tb'wanna loser like :) '\n",
      "1\t0.00\tb'zayn pleas follow would mean world birthday :D love much amaz <3'\n",
      "1\t0.00\tb'congratul friend graduat lose plot :D'\n",
      "1\t0.00\tb'0 sleep offici way pari cannot wait :) pari tdf 2015                '\n",
      "1\t0.00\tb'put trust :)'\n",
      "1\t0.00\tb'hey good suggest guy make minion slot would fun cute :)'\n",
      "1\t0.00\tb\"happi b'day :)\"\n",
      "1\t0.00\tb'hah ... thousand lie :D'\n",
      "1\t0.00\tb'wow look good isabella make wanna start work back :D'\n",
      "1\t0.00\tb'go room bc 3 :-)'\n",
      "1\t0.00\tb\"okeyyy vddd :) ); excit can't wait ... love selfee pic insta :)   special coco ..             dishoom\"\n",
      "1\t0.00\tb'think write :)'\n",
      "1\t0.00\tb'final went drive tonight :-)'\n",
      "1\t0.00\tb'thank follow great day :)'\n",
      "1\t0.00\tb'im go sleep goodnight :)'\n",
      "1\t0.00\tb'hello :)'\n",
      "1\t0.00\tb'yup know okay let tri :) done hhahhaaa  haha'\n",
      "1\t0.00\tb\"i'm suppos ask buy album :)\"\n",
      "1\t0.00\tb'6 day go :)'\n",
      "1\t0.00\tb'hello :) get youth job opportun follow'\n",
      "1\t0.00\tb\"least there' one cool thing :-)\"\n",
      "1\t0.00\tb\"hey guy that' new page :) enjoy\"\n",
      "1\t0.00\tb'jeez tara move teamcannib vs teamspacewhalingisthebest :p'\n",
      "1\t0.00\tb'morn gym session done feel good workout earli ... enjoy rest day :) friyay workout fitfa '\n",
      "1\t0.00\tb'guess never realli identifi pharmaci :) verylaterealis iwishiknewbett'\n",
      "1\t0.00\tb'yeah know lack thing hey pleas check final design satisfi enough :)'\n",
      "1\t0.00\tb'wow nice combin :)'\n",
      "1\t0.00\tb'charg speaker final got earlier today :)'\n",
      "1\t0.00\tb':) masaantoday'\n",
      "1\t0.00\tb'later turn :D'\n",
      "1\t0.00\tb\"text wanna meet i'll get :)\"\n",
      "1\t0.00\tb'long :)'\n",
      "1\t0.00\tb'friend never talk ess-aych-eye-te anyon :)'\n",
      "1\t0.00\tb'ya thank :) '\n",
      "1\t0.00\tb'stat day arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb\"yeah left one supposedli hurt bc right heart appar it'll fine good luck :)\"\n",
      "1\t0.00\tb'hello :) get youth job opportun follow'\n",
      "1\t0.00\tb'ahh thank u :D'\n",
      "1\t0.00\tb\"niall follow fan :) i'm still without follow \"\n",
      "1\t0.00\tb'sister best best pre bday celebr :)'\n",
      "1\t0.00\tb'among time fav tweet :D'\n",
      "1\t0.00\tb'happi birthday g best life :)'\n",
      "1\t0.00\tb'need stress time :)'\n",
      "1\t0.00\tb'immedi plan hope someday futur :)'\n",
      "1\t0.00\tb\"could make foxy' song version instrument form sound epic beat alon :)\"\n",
      "1\t0.00\tb'second well done :)'\n",
      "1\t0.00\tb'think hard bed :p'\n",
      "1\t0.00\tb'tomorrow goldcoast :)'\n",
      "1\t0.00\tb'like way said gal lelomustfal beautiful way :)'\n",
      "1\t0.00\tb'happi save day :) hope enjoy meal sam'\n",
      "1\t0.00\tb'thank min :)'\n",
      "1\t0.00\tb'5g liker :) like fast'\n",
      "1\t0.00\tb\"like morn got dress bought :) i'm total love  newdress happi \"\n",
      "1\t0.00\tb'ff could anyon resist beauti smile :-) quacketyquack'\n",
      "1\t0.00\tb'u mean hide fot :p'\n",
      "1\t0.00\tb'aww thankyou :)'\n",
      "1\t0.00\tb\"ok good night wish troy ugli met today :) ): ok today fun i'm excit tmrw\"\n",
      "1\t0.00\tb'inde :-)'\n",
      "1\t0.00\tb':) good afternoon :D twitterfollowerswhatsup happyfriedday keepsafealway loveyeah emojasp_her'\n",
      "1\t0.00\tb'okay :)'\n",
      "1\t0.00\tb'make two us :D'\n",
      "1\t0.00\tb\"hurt read people' holiday work :D\"\n",
      "1\t0.00\tb'nice hear want switch us :) pro kit would best choic follow link'\n",
      "1\t0.00\tb'beauti ... winter like summer russia :)'\n",
      "1\t0.00\tb'thought  like mate :)'\n",
      "1\t0.00\tb\"i'm play brain dot braindot\"\n",
      "1\t0.00\tb'stat week arriv 1 new follow unfollow :) via'\n",
      "1\t0.00\tb'dm :p'\n",
      "1\t0.00\tb'also come game :)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb':) hope enjoy'\n",
      "1\t0.00\tb'ok first time chat made joke lol believ wont forget u :) name'\n",
      "1\t0.00\tb'mymonsoonstori bodi shop bodi mist vanilla work wonder skin :)'\n",
      "1\t0.00\tb':) best youtub ... keep good work man starsquad sidemen'\n",
      "1\t0.00\tb'yaaayyy best friendaaa shout like minion :)'\n",
      "1\t0.00\tb'thank ad us list :) make sure keep touch news light bulb made corn wast'\n",
      "1\t0.00\tb'1tbps4 wow prize delight eye pick win burst sing ps4 mine feel divin :)'\n",
      "1\t0.00\tb'bath :) even  wheeli bin nowher put'\n",
      "1\t0.00\tb'get readi ... ubericecream messengerforaday :)'\n",
      "1\t0.00\tb'yeap :)'\n",
      "1\t0.00\tb'must watch :D bajrangibhaijaanhighestweek 1'\n",
      "1\t0.00\tb'cool ... congrat kyli u look nice :-)'\n",
      "1\t0.00\tb'thought work season realli busi day ok deal weekend toilet break :p'\n",
      "1\t0.00\tb'daddi af ... :-)'\n",
      "1\t0.00\tb'latest one :)'\n",
      "1\t0.00\tb'happi make :)'\n",
      "1\t0.00\tb'far good ikaw musta cheatmat :)'\n",
      "1\t0.00\tb'e l f thank singer :) kyuhyun thank e l f'\n",
      "1\t0.00\tb'20 ghanton se light nahi :)'\n",
      "1\t0.00\tb'one last long lol block alreadi must nice tweet hate :)'\n",
      "1\t0.00\tb'ur skin think ur still 16 decid break :-) cool'\n",
      "1\t0.00\tb\"easy.get 5:30 go work come home bout 6 take care home famili therein that' see :-)\"\n",
      "1\t0.00\tb'thank majalah juli issu :) love            '\n",
      "1\t0.00\tb'piti parti :-)'\n",
      "1\t0.00\tb'tgif unless one student :) light read singapor hero gp essay alevel'\n",
      "1\t0.00\tb\"dominiqu i'm biggest fan like oh i'm england get fan sign :)\"\n",
      "1\t0.00\tb\"good caus i'd like upload :)\"\n",
      "1\t0.00\tb'goood mornin earthl :D'\n",
      "1\t0.00\tb'happi tweet like new build :) mm'\n",
      "1\t0.00\tb'okay yay ask tomorrow pleas :)'\n",
      "1\t0.00\tb'alway :) posit'\n",
      "1\t0.00\tb'lamp shop :)'\n",
      "1\t0.00\tb'word big site got chang a-foot :) websit develop revamp'\n",
      "1\t0.00\tb'vote brainchild 3 time pleas may poem :D'\n",
      "1\t0.00\tb'confid confin u ur limit ... :)'\n",
      "1\t0.00\tb'wish colorado :)'\n",
      "1\t0.00\tb'happi friday folk order goodyear tyre onlin get upto  40 cashback cs appli yourewelcom :)'\n",
      "1\t0.00\tb'nightli routin simpin :-)'\n",
      "1\t0.00\tb'add snapchat yall :) give name'\n",
      "1\t0.00\tb\"i'm look influenc app :) interest here' invit\"\n",
      "1\t0.00\tb'sketchbook art love 4wild draw hair turn pretti cool :D art hair color colorpencil cray '\n",
      "1\t0.00\tb'tune back hubbi :) u play queen pl'\n",
      "1\t0.00\tb\"fell asleep like 6:30 can't fall asleep two hour :-)\"\n",
      "1\t0.00\tb'great night great peopl :)'\n",
      "1\t0.00\tb'imma use next time :)'\n",
      "1\t0.00\tb'ob 11h v kino :)'\n",
      "1\t0.00\tb\"gone make tho posit day :) adult can't keep act like mom ..\"\n",
      "1\t0.00\tb'hi see u like fourfivesecond think u might like deaf ear plz let know u think :)'\n",
      "1\t0.00\tb'great went kardamena last summer amaz beach wonder sea year time go samo greec :) :)'\n",
      "1\t0.00\tb'chicken caesar salad tad bland live :)'\n",
      "1\t0.00\tb'love afraid respond hate :) ili x x x'\n",
      "1\t0.00\tb'first leg tour go well :D music band rock magictrik tour'\n",
      "1\t0.00\tb'okk den call abp n spread allov india n abroad well :p'\n",
      "1\t0.00\tb'thank much :)'\n",
      "1\t0.00\tb'pleas let us know store abl visit may inform expect :)'\n",
      "1\t0.00\tb'tweet much appreci thank dedic member :)'\n",
      "1\t0.00\tb'freedom hangout whoever want :)'\n",
      "1\t0.00\tb\"i'm like :)\"\n",
      "1\t0.00\tb'hi :-) look tourist info get applic zonzofox click link bye ;-)'\n",
      "1\t0.00\tb'leo season :-)  '\n",
      "1\t0.00\tb'life kutiyapanti :D love life :D'\n",
      "1\t0.00\tb'thank support :) kunoriforceo ceo 1month'\n",
      "1\t0.00\tb'profession hit video wick boomshot fuh di world even thank :)'\n",
      "1\t0.00\tb'yeeey welcom club one half year whatsapp .. donot regret .. :)'\n",
      "1\t0.00\tb'problem :) pleasur  '\n",
      "1\t0.00\tb'send one pleas :) wont expos roast <3'\n",
      "1\t0.00\tb'lemm cover u lipstick n cran vodka :-)'\n",
      "1\t0.00\tb'road trip :D'\n",
      "1\t0.00\tb'alway walk past place may go tri :D'\n",
      "1\t0.00\tb\"yeah i'v lost call day :)\"\n",
      "1\t0.00\tb'jummah mubarak rememb prayr :)'\n",
      "1\t0.00\tb'member       chanc :D'\n",
      "1\t0.00\tb'miss babi :) onemochaonelov'\n",
      "1\t0.00\tb'yo southpaw great movi someon better get award :D'\n",
      "1\t0.00\tb'music human geniu stroma :)'\n",
      "1\t0.00\tb' live younow jonah jareddd :)'\n",
      "1\t0.00\tb'messag full name b address postcod talkmobil phone number thank :)'\n",
      "1\t0.00\tb'last class morn two week break :-)'\n",
      "1\t0.00\tb'happi happi birthday emili :D'\n",
      "1\t0.00\tb'elf fight huha :D :) teenchoic choiceinternationalartist superjunior'\n",
      "1\t0.00\tb'may find confus :) simpl wolf'\n",
      "1\t0.00\tb'nice pic :)'\n",
      "1\t0.00\tb'promot :)'\n",
      "1\t0.00\tb'yeah bird transform sword :D'\n",
      "1\t0.00\tb'even real word :D'\n",
      "1\t0.00\tb'could misread wors thing :p'\n",
      "1\t0.00\tb'forward richard ... follow :)'\n",
      "1\t0.00\tb'hey like fnaf check youtub channel :) via'\n",
      "1\t0.00\tb'yup happen everywher peopl cant debat come name call :-)'\n",
      "1\t0.00\tb'need get work today ibiza :) :) :) birthdaymoneyforjesusjuic'\n",
      "1\t0.00\tb'anytim :)'\n",
      "1\t0.00\tb\"mistak happen man long get play game we'll happi :)\"\n",
      "1\t0.00\tb'hey wanna check ytb channel post gameplay tutori :) via'\n",
      "1\t0.00\tb'one construct critic welcom :) ganesha textur photographi art hinduism hindugod elephantgod'\n",
      "1\t0.00\tb'ye alway selfish fuck :)'\n",
      "1\t0.00\tb'hey like fnaf check youtub channel :) via'\n",
      "1\t0.00\tb'want creat first bboy cardgam pixelart gamedesign :D think indiedev pixel_daili gamedev'\n",
      "1\t0.00\tb'sound still like good idea :)'\n",
      "1\t0.00\tb'limit plateau must stay must go beyond :) bruce lee'\n",
      "1\t0.00\tb'man sing rain :)'\n",
      "1\t0.00\tb'laguna :) see'\n",
      "1\t0.00\tb':) time nhi tha offic work bahot tha 10 baje raat ko liya tha hath tri creat best 2 ghant itna hi bana paya'\n",
      "1\t0.00\tb\"that' true quit whenev want :D\"\n",
      "1\t0.00\tb'thank :) weekend tri best :)'\n",
      "1\t0.00\tb'u like uta read manga :-) love even :-) haha yeah'\n",
      "1\t0.00\tb'famili hang :)  feel festiv jamuna futur park'\n",
      "1\t0.00\tb'move forward happi spirit find thing alway work \\\\:'\n",
      "1\t0.00\tb'rememb sent present last christma dub swiftma :)'\n",
      "1\t0.00\tb'miss :-)'\n",
      "1\t0.00\tb'super proud :)'\n",
      "1\t0.00\tb'love pic fab show :-) x'\n",
      "1\t0.00\tb'morn :)'\n",
      "1\t0.00\tb'tha case trion interact forum twitch :)'\n",
      "1\t0.00\tb\"that' spirit :) wsalelov\"\n",
      "1\t0.00\tb'sort earli hour thank help rais initi issu make commun :)'\n",
      "1\t0.00\tb\"hii follow pleas i'd like ask one thing dm :) \"\n",
      "1\t0.00\tb'happi b-day :)'\n",
      "1\t0.00\tb\"that' disgust glad acc back :) hope get deserv\"\n",
      "1\t0.00\tb'back 80 brand new commodor 64 program commun town :) 12th august'\n",
      "1\t0.00\tb'okay :)'\n",
      "1\t0.00\tb'im wear mine dj next fri :-)'\n",
      "1\t0.00\tb\"thank take time tweet us annabel we'll pass london bridg team :)\"\n",
      "1\t0.00\tb'alreadi :)'\n",
      "1\t0.00\tb'everi way like jess :)'\n",
      "1\t0.00\tb'happi friday :)'\n",
      "1\t0.00\tb'short routin quest borderland :) wanderrook'\n",
      "1\t0.00\tb\"thank made day :) let' organis meet soon base london\"\n",
      "1\t0.00\tb\"i'm person workout eat healthi minut gain weight :-)\"\n",
      "1\t0.00\tb'gm meet preciou soft ladi real friend mizz preciou :) ;) mani like ...'\n",
      "1\t0.00\tb'aameen .. :) long live pakistan .. bleedgreen ..   '\n",
      "1\t0.00\tb'alreadi follow back oppa :)'\n",
      "1\t0.00\tb'liam sophia chicago :) '\n",
      "1\t0.00\tb'btw honeymoon :)'\n",
      "1\t0.00\tb\"yeah bring boom da'esh co-ord fsa estat :)\"\n",
      "1\t0.00\tb\"welcom :) when' next meet\"\n",
      "1\t0.00\tb'singer dusti tunisia <3 music tunisia <3 wonder nice peopl :-) <3 youtub ...'\n",
      "1\t0.00\tb\"la class' :p\"\n",
      "1\t0.00\tb'hahaha go doug :-)'\n",
      "1\t0.00\tb'forgot ask may post fb happi friday :-)'\n",
      "1\t0.00\tb'visit blog thank :D'\n",
      "1\t0.00\tb'everyon fuckin irrit :)'\n",
      "1\t0.00\tb'also mean imma go back twitter activ :D caus know everyon miss ;) xd'\n",
      "1\t0.00\tb\"hello ladi apolog delay pleas dm us address size detail we'll ship prize :)\"\n",
      "1\t0.00\tb'free voucher twitter friend use fiverr :)'\n",
      "1\t0.00\tb\"oh gina busi :-) can't wait see hear xxx soproud\"\n",
      "1\t0.00\tb'happi friday everyon hope fantast week :) friday'\n",
      "1\t0.00\tb'ye one team :)'\n",
      "1\t0.00\tb'happi belat birthday sweeti :)'\n",
      "1\t0.00\tb'morn emma :)'\n",
      "1\t0.00\tb'thank guy :)'\n",
      "1\t0.00\tb'would say :-)'\n",
      "1\t0.00\tb'love make two heart one :)'\n",
      "1\t0.00\tb'thank ad us list :) make sure keep touch news light bulb made corn wast'\n",
      "1\t0.00\tb'thank mr bee xxx :-)'\n",
      "1\t0.00\tb'thankyou :)'\n",
      "1\t0.00\tb'close enought :)'\n",
      "1\t0.00\tb'good luck ... anoth potenti favourit water hole :-)'\n",
      "1\t0.00\tb'bad boy :) burger melbourneburg'\n",
      "1\t0.00\tb'want ft arianna :-)'\n",
      "1\t0.00\tb'everyon follow :)'\n",
      "1\t0.00\tb'hey lesley sorri get shall send post tonight :)'\n",
      "1\t0.00\tb'thank esai :-)'\n",
      "1\t0.00\tb\"thank text back :) i'm text tomorrow :)\"\n",
      "1\t0.00\tb'unfollow back :)'\n",
      "1\t0.00\tb'hope enjoy stay rotterdam know cheer jordi clasi :)'\n",
      "1\t0.00\tb'hi hot girl say hot horni darl xx :) '\n",
      "1\t0.00\tb'sure done :)'\n",
      "1\t0.00\tb'salon bleach hair olaplex damag like :)'\n",
      "1\t0.00\tb'teamwork right :D zitecofficestori'\n",
      "1\t0.00\tb'ff happyfriday great friday :-)'\n",
      "1\t0.00\tb'sure :)'\n",
      "1\t0.00\tb\" got 7 let' got 7 fact :)\"\n",
      "1\t0.00\tb'still tomorrow work colleagu interest bring kid along :) point eb page'\n",
      "1\t0.00\tb'tack <3 :D'\n",
      "1\t0.00\tb\"t'would tweetup without book rest assur book :)\"\n",
      "1\t0.00\tb'thank much know greatest magic detect jonathancreek back tv set dvr :)'\n",
      "1\t0.00\tb'get home 4 wake 9 :) :)'\n",
      "1\t0.00\tb'thank take time tweet kat :)'\n",
      "1\t0.00\tb'lot rarer life twitter :)'\n",
      "1\t0.00\tb'okkk frend milt h break k bad :)'\n",
      "1\t0.00\tb'could fun sexi girl two sexi girl :)   '\n",
      "1\t0.00\tb'.. thank mario ... wonder friday ... :)'\n",
      "1\t0.00\tb'fun daniel hope goe well :-)'\n",
      "1\t0.00\tb'thank hope got good book keep compani :-)'\n",
      "1\t0.00\tb\"yeah i'v watch cours rewatch ad free faster hope :) thank look forward\"\n",
      "1\t0.00\tb\"price around 1600 i'm gonna forward info email decid interest :)\"\n",
      "1\t0.00\tb'got bed yet :-)'\n",
      "1\t0.00\tb\"awww sige next time u know na punta kayo dun imma make sure na we'll go din :-)\"\n",
      "1\t0.00\tb'sure thing :) x'\n",
      "1\t0.00\tb'... friday :D'\n",
      "1\t0.00\tb'cours would like :) x'\n",
      "1\t0.00\tb'nooo didnt even know that actual type googl :)'\n",
      "1\t0.00\tb'found prompt :-)'\n",
      "1\t0.00\tb'chillin w salmon :)'\n",
      "1\t0.00\tb'see beauti believ exist first :)'\n",
      "1\t0.00\tb\"hi j upgrad t-mobil orang ee us dm mobil number i'll ask sale call :)\"\n",
      "1\t0.00\tb\"congratul realli look forward book good sunday morn view that' awesom teapot btw :-) x\"\n",
      "1\t0.00\tb'three refresh cocktail even  hotter  summer one suit :) londoutrad cocktail'\n",
      "1\t0.00\tb'love :) <3'\n",
      "1\t0.00\tb\"wip let get thing tweak i'll link :) max kal tak\"\n",
      "1\t0.00\tb'fab giveaway :)'\n",
      "1\t0.00\tb'case .. facebook wall :p'\n",
      "1\t0.00\tb'stat week arriv 3 new follow unfollow :) via'\n",
      "1\t0.00\tb'home alon :D'\n",
      "1\t0.00\tb'wish happi birthday awesom fun fill day :D'\n",
      "1\t0.00\tb'let know hear anyth :) xx'\n",
      "1\t0.00\tb'wayward pine later :)'\n",
      "1\t0.00\tb'yeah better use offici account :) like  '\n",
      "1\t0.00\tb'rock muscl :D ilikeit'\n",
      "1\t0.00\tb'far lfc fan make expert spot mental weak lack consist :-)'\n",
      "1\t0.00\tb'check page belong one watford commun hous trust enterpris cube particp :-) ...'\n",
      "1\t0.00\tb'follow back pleass :)'\n",
      "1\t0.00\tb'omg that sweet thing say thank :-) yeah use go saudi arabia lot dad'\n",
      "1\t0.00\tb'ty followfriday top influenc commun week :)'\n",
      "1\t0.00\tb'good luck ur exam :)'\n",
      "1\t0.00\tb'one approv recogn fanbas bailona :)'\n",
      "1\t0.00\tb'nice see peopl chang need fix care :)'\n",
      "1\t0.00\tb'happi responsibilti ask other make happi :) fb'\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'happi :) <3'\n",
      "1\t0.00\tb'good night tiger sweet furri dream xxoo'\n",
      "1\t0.00\tb'nice one :)'\n",
      "1\t0.00\tb'want elev posit want promot want favor god favor men ... :)'\n",
      "1\t0.00\tb\"i'm insecur tonight :-)\"\n",
      "1\t0.00\tb'<3 guy :)'\n",
      "1\t0.00\tb'dan god bless meet greet soon :)'\n",
      "1\t0.00\tb'mom :) horror movi'\n",
      "1\t0.00\tb\"oh hell yeah :) i'll expect text next tuesday\"\n",
      "1\t0.00\tb'welcom sabah :-)'\n",
      "1\t0.00\tb'match day bitchesss real madrid vs man shitti :D'\n",
      "1\t0.00\tb'squash commit still make 200 commit :)'\n",
      "1\t0.00\tb'west countri weekend next ride day tuesday :) sunni weekend around'\n",
      "1\t0.00\tb'ourdaughtersourprid mani mani congratul papa ji :)'\n",
      "1\t0.00\tb'thank becca :)'\n",
      "1\t0.00\tb'love new song delta rock :)'\n",
      "1\t0.00\tb'saw thought might need upgrad girl get bigger :)'\n",
      "1\t0.00\tb'first love wanna fuck late night think got nut :-) v look ...'\n",
      "1\t0.00\tb'enjoy cute babi panda :)'\n",
      "1\t0.00\tb'done yun :)'\n",
      "1\t0.00\tb'watch joe dirt 2 :)'\n",
      "1\t0.00\tb'ohh happyfriday thank love team :)'\n",
      "1\t0.00\tb'alway posit :) postiv selfi'\n",
      "1\t0.00\tb'good morn sharon realli hope better today medicin work thank :) ttyl x'\n",
      "1\t0.00\tb'oh fab gav love love ladi linda talk time great actress :D x'\n",
      "1\t0.00\tb'follback :D'\n",
      "1\t0.00\tb'answer super sonic fast :) would love win first time ;) gohf'\n",
      "1\t0.00\tb'3hr .. :) tym prepar'\n",
      "1\t0.00\tb'dieback music pack total worth awesom :) dota 2'\n",
      "1\t0.00\tb'morn :)'\n",
      "1\t0.00\tb'think might endit london photo vid minecon london video :) look rememb great time'\n",
      "1\t0.00\tb'ok ... sere 2 play yah min .. haha :)'\n",
      "1\t0.00\tb'joerin scene :) joshan power tandem'\n",
      "1\t0.00\tb\":D can't sleep need tri lay bed bore\"\n",
      "1\t0.00\tb'stat day arriv 6 new follow unfollow :) via'\n",
      "1\t0.00\tb'b call friend need :) ...'\n",
      "1\t0.00\tb'hello ligao citi albay :-) concert bcyc lnh'\n",
      "1\t0.00\tb'goodnight love luke heart :-) love '\n",
      "1\t0.00\tb'happi birthday enjoy vacat :)'\n",
      "1\t0.00\tb'congratul sat 3rd honorari degre everyon alac :)'\n",
      "1\t0.00\tb'hey raspberri island skelo_ghost :) want get iph 0ne 6 free kindli check bi0 thx'\n",
      "1\t0.00\tb'thank u :)'\n",
      "1\t0.00\tb'thank great :)   '\n",
      "1\t0.00\tb'deserv madadagdagan pa yan :-) congrat :-) bmc <3'\n",
      "1\t0.00\tb'save money use good way money money better life fun :) nice day 11:11'\n",
      "1\t0.00\tb'u find ur friend ditch u caus lot thing :) wtf done im actual nice person'\n",
      "1\t0.00\tb'order dress .. :) :)'\n",
      "1\t0.00\tb'like eye :D'\n",
      "1\t0.00\tb'dear person pleas studi embarrass urself entropi work 100 w evolut :)'\n",
      "1\t0.00\tb'ye stay touch :) instagram'\n",
      "1\t0.00\tb'look forward alway love giggl loop two great act :)'\n",
      "1\t0.00\tb'cheer mention ... even wrong section :)'\n",
      "1\t0.00\tb\"ty dear eva i'm total agre :-) keep smile u\"\n",
      "1\t0.00\tb'spain morn :) good morn ian'\n",
      "1\t0.00\tb'anoth note found camden town ... :D work'\n",
      "1\t0.00\tb'even talk great :)'\n",
      "1\t0.00\tb'ohhh wait 28 :) luv uhh'\n",
      "1\t0.00\tb'thank favorit :) make sure keep touch news light bulb made corn wast'\n",
      "1\t0.00\tb'scoup 17 like jren nuest :D'\n",
      "1\t0.00\tb'oh love lovelayyy thank ok kidney .. want anyway :p'\n",
      "1\t0.00\tb'neuer post onlin :) fix spray'\n",
      "1\t0.00\tb'check us :) untangl never easier dtangl patent innov'\n",
      "1\t0.00\tb'class today train friend :)'\n",
      "1\t0.00\tb'okay let get bat :)'\n",
      "1\t0.00\tb'one fall love choic  chanc ... :) '\n",
      "1\t0.00\tb'would email donnae.strydom@westerncape.gov.za :-) ta'\n",
      "1\t0.00\tb\"that' great hear we'll sure pass thank staff :) nic\"\n",
      "1\t0.00\tb'seen uni .. talent :)'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "1\t0.00\tb'aww love kind men :)'\n",
      "1\t0.00\tb'mca money tell stori :)'\n",
      "1\t0.00\tb'save live :-)'\n",
      "1\t0.00\tb'hey juaquin v3nzor99 :) want get iph 0ne 6 free kindli check bi0 thx'\n",
      "1\t0.00\tb'break shell :-)'\n",
      "1\t0.00\tb'truth :D'\n",
      "1\t0.00\tb'fantast thank :)'\n",
      "1\t0.00\tb'heyi :) u rt link tag michael pleas thank'\n",
      "1\t0.00\tb'hashtag say ... :)'\n",
      "1\t0.00\tb'still green tea blend flavor :)'\n",
      "1\t0.00\tb\"i'm excit what' come :) realli deserv\"\n",
      "1\t0.00\tb'today improv tomorrow ... :-) :)'\n",
      "1\t0.00\tb'sometim :-)'\n",
      "1\t0.00\tb\"i'm sorri ran friend rough day i'll tri stream saturday :)\"\n",
      "1\t0.00\tb'get well soon :) xx'\n",
      "1\t0.00\tb'bed best coupl ever :)'\n",
      "1\t0.00\tb'favorit appl sauc :)'\n",
      "1\t0.00\tb'nice holiday honey :-) kiss'\n",
      "1\t0.00\tb'follow back pleass :)'\n",
      "1\t0.00\tb'thakyou sir :)'\n",
      "1\t0.00\tb'thank beatriz :)'\n",
      "1\t0.00\tb'rt bailona group chat mention approv fanbas :)'\n",
      "1\t0.00\tb'parti cancel :p bajrangibhaijaanhighestweek 1'\n",
      "1\t0.00\tb'thank follow :-) hope great week'\n",
      "1\t0.00\tb'puff pastri egg tart hot fresh oven :) wan chai mtr'\n",
      "1\t0.00\tb'thank follow us :) like cool new product check campaign'\n",
      "1\t0.00\tb'make alyssa rub tummi :)'\n",
      "1\t0.00\tb'almost done master sword :D princess zelda'\n",
      "1\t0.00\tb'ive email regard cours queri :)'\n",
      "1\t0.00\tb'happi birthday  jiva ever :)   '\n",
      "1\t0.00\tb'awesom news mate well happi :)'\n",
      "1\t0.00\tb'jumma mubbarak :)  '\n",
      "1\t0.00\tb'gorgeou deborah good tast :) use coupon code colourdeb red purpl blue thank'\n",
      "1\t0.00\tb\"im sure wasnt anyth we'r alreadi use :) glad ur home\"\n",
      "1\t0.00\tb'wooo happi friday friend :) ff'\n",
      "1\t0.00\tb\"plan send email chippy' vessel monday ps happi anniversari haha :)\"\n",
      "1\t0.00\tb'comput sex count :) funni vintag'\n",
      "1\t0.00\tb\"use i'll pick next time i'm :) yeah i'll cook nice big kitchen  \"\n",
      "1\t0.00\tb'love great collag :)'\n",
      "1\t0.00\tb'       thank alway put smile face mind follow :) nice day x 1710'\n",
      "1\t0.00\tb'look fun snapchat gooffeanotter snapchat kiksex snapm lesbian instagram mpoint mugshot :)'\n",
      "1\t0.00\tb\"can't sleep much want love bug made 33 token never mind :)\"\n",
      "1\t0.00\tb'gamer follow train follow retweet follow retweet gain activ follow :) 110'\n",
      "1\t0.00\tb'look forward hear thought :) maritimen'\n",
      "1\t0.00\tb'stat day arriv 2 new follow unfollow :) via'\n",
      "1\t0.00\tb\"there' stun show garden rh tatton park show wonder design construct plant :)\"\n",
      "1\t0.00\tb'behav els u jump_julia malema :D'\n",
      "1\t0.00\tb'hello :) get youth job opportun follow'\n",
      "1\t0.00\tb'great friday fren :-)'\n",
      "1\t0.00\tb'nuf teas alreadi show us alien everyday seem get closer see alien cousin :-)'\n",
      "1\t0.00\tb'new monitor good :D'\n",
      "1\t0.00\tb'good morn kimmi :)'\n",
      "1\t0.00\tb\"channel' show planetbollywoodnew full epi avail onlin :)\"\n",
      "1\t0.00\tb'evolut :D'\n",
      "1\t0.00\tb'fback :)'\n",
      "1\t0.00\tb'love airport :-) :-)'\n",
      "1\t0.00\tb'love :) want corn chip :)'\n",
      "1\t0.00\tb'thank :) tricki bt v import subject'\n",
      "1\t0.00\tb'hah ... :D say sorri ...'\n",
      "1\t0.00\tb'be-shak :p'\n",
      "1\t0.00\tb':)'\n",
      "1\t0.00\tb'happi birthday miss chenoweth hope great one ... oodl cake :)'\n",
      "1\t0.00\tb\"happi birthday gorgeou hope i'll see fair :)\"\n",
      "1\t0.00\tb'im go bed love hailey milk crai sxxx cl runway look :)'\n",
      "1\t0.00\tb'feel someon share review work hard :D'\n",
      "1\t0.00\tb'gooodnight love boy iv ri zoe kati jayci jen :)'\n",
      "1\t0.00\tb\"well bud guess i'm go bed goodnight guy see morn whatev woke :)\"\n",
      "1\t0.00\tb'thank follow great day :)'\n",
      "1\t0.00\tb'thank :)'\n",
      "1\t0.00\tb'bad thing think weird face rad :-) thank though'\n",
      "1\t0.00\tb'make aliv make suffer make feel .. addict song alway sing karaok :-) ..'\n",
      "1\t0.00\tb'realli cuti :D'\n",
      "1\t0.00\tb'ltsw gooo :-)'\n",
      "1\t0.00\tb'thank favorit :) make sure keep touch news light bulb made corn wast'\n",
      "1\t0.00\tb'threw phone wall :)'\n",
      "1\t0.00\tb'giant balloon want one :)'\n",
      "1\t0.00\tb'face time :)'\n",
      "1\t0.00\tb\"what' hashtag weekend guy :)\"\n",
      "1\t0.00\tb\"one look like need much care studi i'll favourit later :-)\"\n",
      "1\t0.00\tb'welcom hun :) thank follow'\n",
      "1\t0.00\tb'12pm :)'\n",
      "1\t0.00\tb'thank much mom get ticket :)'\n",
      "1\t0.00\tb\"thank manila_bro follow let' connect soon :)\"\n",
      "1\t0.00\tb'lol sometim tweet account lol day home dont lot say :D'\n",
      "1\t0.00\tb'       thank alway put smile face mind follow :) nice day x 1709'\n",
      "1\t0.00\tb\"hope one day he'll fall love strong woman let see thing refus see :)\"\n",
      "1\t0.00\tb'actual lol :)'\n",
      "1\t0.00\tb\"that' great idea love fellow veggi :-)\"\n",
      "1\t0.00\tb'c 75 made collagen an 2win hopetowin :)'\n",
      "1\t0.00\tb'awww thank hey million :) enjoy weekend hey'\n",
      "1\t0.00\tb'guess need build content around imag inventori littl bit :)'\n",
      "1\t0.00\tb'gamer follow train follow retweet follow retweet gain activ follow :) 1'\n",
      "1\t0.00\tb'snap :)'\n",
      "1\t0.00\tb'bring togeth friend famili loveforfood foodforthought thoughtfortheday :)'\n",
      "1\t0.00\tb'carp diem mr nath :)'\n",
      "1\t0.00\tb\"yep i'd without ning recommend book thought wow :) think gener posit lead\"\n",
      "1\t0.00\tb'ty appreci recent retweet great friday :)'\n",
      "1\t0.00\tb\"11:11 meet michael hug tight talk tell he' import love much make smile :D\"\n",
      "1\t0.00\tb'long feel comfort im gonna wear want mother haha ... sound nice ... :-)'\n",
      "1\t0.00\tb'congratul daughter :D'\n",
      "1\t0.00\tb'goodnight love everyon hate stupid :)'\n",
      "1\t0.00\tb'thank fill although opinion still stand agre disagre harm done :)'\n",
      "1\t0.00\tb\"see saturday :p i'll see stormi :D\"\n",
      "1\t0.00\tb'time 2 parti :D'\n",
      "1\t0.00\tb'upgrad an sync plu done singl remot devic get folk :)'\n",
      "1\t0.00\tb'haha know mess :D'\n",
      "1\t0.00\tb'want say huge thank ff thank support :)'\n",
      "1\t0.00\tb'found song mention nylon gvb cd take mountain.titl unto other theworldwouldchang :)'\n",
      "1\t0.00\tb'relat would make new categori blog everyth one place :)'\n",
      "1\t0.00\tb\"yah know i'm good fake mah emot :)\"\n",
      "1\t0.00\tb'go nice mother daughter day today :) even though rain morn great day folk xx'\n",
      "1\t0.00\tb'idk cant wait tonight panel peopl :)'\n",
      "1\t0.00\tb':) reason dream ...'\n",
      "1\t0.00\tb\"i'am elf hbu :)\"\n",
      "1\t0.00\tb'follow back pleass :)'\n",
      "1\t0.00\tb'ps took 80-1 hope count :-)'\n",
      "1\t0.00\tb'       thank alway put smile face mind follow :) nice day x 1708'\n",
      "1\t0.00\tb'yeah neenkin congrat success masterpiec :) '\n",
      "1\t0.00\tb'goodmorn smile debit order :) soon'\n",
      "1\t0.00\tb'thank work beagl amaz incred :)'\n",
      "1\t0.00\tb'rt bailona group chat mention approv fanbas :)'\n",
      "1\t0.00\tb'rememb someon :)  see feat charli puth wiz khalifa '\n",
      "1\t0.00\tb'cours :)'\n",
      "1\t0.00\tb'hi dylan tyler :)   '\n",
      "1\t0.00\tb'favorit tea :)'\n",
      "1\t0.00\tb\"i'v done today watch law order svu love sick :-)\"\n",
      "1\t0.00\tb'pleasur :D enjoy day'\n",
      "1\t0.00\tb'fine day :)'\n",
      "1\t0.00\tb'morn wear get darker end world day today instead septemb :-)'\n",
      "1\t0.00\tb'lmao love berni :D'\n",
      "1\t0.00\tb'lol ... morn :) ... wet hey friday :)'\n",
      "1\t0.00\tb'aint never upload lol also found anoth vid u danc henri trap keep go fit journey tommi :)'\n",
      "1\t0.00\tb\"ye think b stupid discuss abt wild card entri jst wait vivian' perform :)\"\n",
      "1\t0.00\tb'new blog import life lesson learnt watch movi :) someth everyon need rememb xx'\n",
      "1\t0.00\tb\"i'm best perfect promis love whole heart :)\"\n",
      "1\t0.00\tb'u transpar ... glass .. :-)'\n",
      "1\t0.00\tb'thank share bitcoin news stream great monday :) insight h'\n",
      "1\t0.00\tb'ye :D'\n",
      "1\t0.00\tb'thank retweet :) make sure keep touch news light bulb made corn wast'\n",
      "1\t0.00\tb\"rang sick yesterday told i'd back today i'm better morn anyway day sunday :)\"\n",
      "1\t0.00\tb\"ping i'm watch tl like hawk :D\"\n",
      "1\t0.00\tb'make use masquerad mask lol zorroreturm :-)'\n",
      "1\t0.00\tb'hope help :-)'\n",
      "1\t0.00\tb'congrat ryan jami delight :) xx'\n",
      "1\t0.00\tb'       thank alway put smile face mind follow :) nice day x 1707'\n",
      "1\t0.00\tb'thank mention happi friday :)'\n",
      "1\t0.00\tb'ye pk hay :p'\n",
      "1\t0.00\tb\"hi jacquelin we'r sorri hear pleas dm us contact detail give call :)\"\n",
      "1\t0.00\tb'passion beer ultim made way becom full-fledg busi :) entrepreneur startup'\n",
      "1\t0.00\tb'thank guy :) fun :) anoth 1 ;)'\n",
      "1\t0.00\tb\"hi i'm see workplac one venu list i'm sure fix thank :)\"\n",
      "1\t0.00\tb'oh wow thank :) skin go perfect time head lago travel'\n",
      "1\t0.00\tb'wow luxord look realli amaz new kingdom heart 3 promo art :)'\n",
      "1\t0.00\tb'new potato garden hundr dig :)'\n",
      "1\t0.00\tb'love :D'\n",
      "1\t0.00\tb'thank love weekend everyon :-)'\n",
      "1\t0.00\tb'well total blew mind morn'\n",
      "1\t0.00\tb\"cours i'v cite academ paper feel i'v arriv :-)\"\n",
      "1\t0.00\tb'life smile :)'\n",
      "1\t0.00\tb'thank info :)'\n",
      "1\t0.00\tb'sure watch pokiri 1nenokkadin tym fav :-) bajrangibhaijaanhighestweek 1'\n",
      "1\t0.00\tb'yang retweet :) :D'\n",
      "1\t0.00\tb\"who' still awak :)\"\n",
      "1\t0.00\tb'heritag sword made wood :D'\n",
      "1\t0.00\tb'right thx andrew great day :)'\n",
      "1\t0.00\tb'true :)'\n",
      "1\t0.00\tb'beleaf u :-)'\n",
      "1\t0.00\tb'cool video love ... :) thank u'\n",
      "1\t0.00\tb'season 11 set  nice friday spnfamili :) spn 11 alwayskeepfight jaredpadalecki jensenackl'\n",
      "1\t0.00\tb'cool :D'\n",
      "1\t0.00\tb'exactli :D'\n",
      "1\t0.00\tb'peasant seat watch peasant team ... mind :p ahahha'\n",
      "1\t0.00\tb\"chri that' great hear :) due time remind inde plan avail distant futur\"\n",
      "1\t0.00\tb'thank shout-out :) great aboard'\n",
      "1\t0.00\tb'hey :) long time talk ...'\n",
      "1\t0.00\tb'matt would say welcom adulthood ... :)'\n",
      "1\t0.00\tb'could say egg face :-)'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Predict with your own Tweet\n",
    "\n",
    "In this part you can predict the sentiment of your own tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03245317766963299\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. See you next week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
